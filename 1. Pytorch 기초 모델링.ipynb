{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기초 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OppeqQLGPDZC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "g4h0Nx96PPdC",
    "outputId": "89282a94-bf6c-4dfe-c09d-c087d564add7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d6da38a790>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVF7M_VrPSzl"
   },
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[2],[4],[6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Jz7TyuEI1Js"
   },
   "source": [
    "## to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "D_5bTI0_Ilqs",
    "outputId": "2010e0e0-0533-43c9-f3f5-7bccc37c11dd",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 기기로 학습합니다: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available() # GPU를 사용가능하면 True, 아니라면 False를 리턴\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") # GPU 사용 가능하면 사용하고 아니면 CPU 사용\n",
    "print(\"다음 기기로 학습합니다:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 고정\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 을 묶는 텐서를 형성한다.\n",
    "\n",
    "Each sample will be retrieved by indexing tensors along the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset \n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "# Tensor Dateset 은 기본적으로 텐서를 입력으로 받는다 (즉 np,pd 같은 데이터형태를 torch.Tensor 로 바꾸어주어야한다.)\n",
    "# Tensor 데이터셋 선언 (x,y 엮어서 하나로 만들어준다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(dataset) 을 해보면 메서드/attribute 들이 형성된것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단하게 생각하면 DataLoader 객체는 학습에 쓰일 데이터 전체를 보관했다가, train 함수가 batch 하나를 요구하면 batch size 개수만큼 데이터를 꺼내서 준다고 보면 된다.\n",
    "\n",
    "실제로 [batch size, num]처럼 미리 잘라놓는 것은 아니고, 내부적으로 Iterator에 포함된 Index가 존재한다. train() 함수가 데이터를 요구하면 사전에 저장된 batch size만큼 return하는 형태이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch_size : 2의 배수를 넣는게 cpu/gpu 구조상 더 효율적이다.\n",
    "- shuffle : 셔플하지 않으면 데이터셋의 순서에 익숙해져서 학습이 비효율적이 될 수 있다. 그래서 True 를 권장 ( 사람도 같은 문제지를 계속 풀면 어느순간 문제의 순서에 익숙해질 수 있다. (즉 데이터가 학습되는 순서에 익숙해져서 학습될수도..)\n",
    "- drop last : drop_last를 하는 이유를 이해하기 위해서 1,000개의 데이터가 있다고 했을 때, 배치 크기가 128이라고 해봅시다. 1,000을 128로 나누면 총 7개가 나오고 나머지로 104개가 남습니다. 이때 104개를 마지막 배치로 한다고 하였을 때 128개를 충족하지 못하였으므로 104개를 그냥 버릴 수도 있습니다. 이때 마지막 배치를 버리려면 drop_last=True를 해주면 됩니다. 이는 다른 미니 배치보다 개수가 적은 마지막 배치를 경사 하강법에 사용하여 마지막 배치가 상대적으로 과대 평가되는 현상을 막아줍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2pk8p3UAfOn"
   },
   "outputs": [],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2nUf1KlFAjCG"
   },
   "outputs": [],
   "source": [
    "dataset = TensorDataset(x_train, y_train) # 텐서데이터셋 형태로 만든다 (X,y 묶어서 하나로 만들어준다.)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "model = nn.Linear(3,1) # 모델설정\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)  # 옵티마이저 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bjbyOeu2BeUR",
    "outputId": "35d5f945-b2fd-42c8-b2b4-ebcef8eb50fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/3 Batch 1/3 Cost: 6299.984863\n",
      "Epoch    0/3 Batch 2/3 Cost: 1424.307251\n",
      "Epoch    0/3 Batch 3/3 Cost: 352.370911\n",
      "Epoch    1/3 Batch 1/3 Cost: 158.049103\n",
      "Epoch    1/3 Batch 2/3 Cost: 99.115723\n",
      "Epoch    1/3 Batch 3/3 Cost: 25.065502\n",
      "Epoch    2/3 Batch 1/3 Cost: 13.175940\n",
      "Epoch    2/3 Batch 2/3 Cost: 0.309332\n",
      "Epoch    2/3 Batch 3/3 Cost: 0.005093\n",
      "Epoch    3/3 Batch 1/3 Cost: 3.167757\n",
      "Epoch    3/3 Batch 2/3 Cost: 1.937045\n",
      "Epoch    3/3 Batch 3/3 Cost: 1.109085\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 3\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader): # batch index , sample( tuple 상태인 x,y ) 를 도출\n",
    "        x_train, y_train = samples # x,y 나누기\n",
    "        y_pred = model(x_train)\n",
    "        cost = F.mse_loss(y_pred, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format( epoch, nb_epochs, batch_idx+1, len(dataloader), cost.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custum Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 과정이 같은 데이터들에 대해서 계속 전처리 과정을 써주는것은 매우 있다. 전처리과정 ~ tensor 형태의 데이터셋 변환 까지 합쳐진 과정을 한번에 수행하는 class 를 정의함으로서 훨씬 효율적으로 작업이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module을 상속하는 Custom Model처럼, Custom DataSet은 torch.utils.data.Dataset를 상속해야 한다. 또한 override해야 하는 것은 다음 두 가지다.\n",
    "\n",
    "__len__(self): dataset의 전체 개수를 알려준다.\n",
    "\n",
    "__getitem__(self, idx): parameter로 idx를 넘겨주면 idx번째의 데이터를 x,y 형태로 반환한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataset(Dataset): \n",
    "    def __init__(self):   #데이터셋의 전처리를 해주는 부분\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "        \n",
    "    def __len__(self):   #데이터셋 길이(총 샘플의 수) 를 도출하는 매직 메서드를 정의한다.\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):   # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴(#데이터셋에서 특정 1개의 샘플을 가져오는 함수)하는 메서드를 정의한다.\n",
    "        x = torch.FloatTensor(self.x_data[idx]) # dataset[i]을 했을 때 i번째 샘플을 가져오도록 하는 인덱싱을 위한 get_item\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y\n",
    "\n",
    "dataset = CustomDataset() # 데이터셋을 원래 형태에서 전처리->pytorch 데이터 형태로 바로 변환이 가능하다.\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHiyuYIDRYHU"
   },
   "source": [
    "## 가중치와 편향의 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Qa5AzJ09RbQA",
    "outputId": "cc955ae7-bfba-4970-e6a1-1dc34e9888c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(1,requires_grad=True) ; print(W)\n",
    "# 가중치를 0 으로 초기화한다. (zeros)\n",
    "# 학습을 통해 값이 변경되는 변수임을 명시한다. (requires_grad = True 임을 통해서)\n",
    "# size 가 1 인 zeros 의 tensor 를 형성하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "WYmGp9OAR_7A",
    "outputId": "2c7faef3-483d-48c8-c018-52cfeaa79d90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "b = torch.zeros(1,requires_grad=True) ; print(b)\n",
    "# 편향(b) 역시 0으로 초기화하고, 학습을 통해서 변하는 변수임을 명시해주자.\n",
    "# size 가 1 인 zero 의 tensor 를 형성하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dTLz-ypmgOZu"
   },
   "source": [
    "## y_pred 모델 형성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "Bo3vti1vg8Iz",
    "outputId": "2148e1db-4112-450e-cedf-4b25e2f3b2e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# linear regression 이므로 y_pred 모델은 아래와 같다.\n",
    "y_pred = X_train*W + b ; print(y_pred) \n",
    "# 처음의 값은 아직 gradient method 를 쓰지 않아서 초기화된 W,b 를 계산한 0,0,0 임을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NYNLH9XSifkF"
   },
   "source": [
    "## cost 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lWbo4bmjBCz"
   },
   "outputs": [],
   "source": [
    "cost = torch.mean((y_pred - y_train)**2)\n",
    "# cost 를 정해준다. 이 떄의 cost 는 mse\n",
    "# 어짜피 계산하는 부분이므로 굳이 변한다는(required_grad = True) 는 쓰지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BiG9UqYljhAt"
   },
   "source": [
    "## optimizer \n",
    "- 맨 앞 인자는 UPDATE 의 대상을 LIST 형태로 받는다.\n",
    "- lr : 학습율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bq1Wqv5cjy6A"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([W,b] ,lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tm2gjV1rj7dr"
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "# optimizer.zero_grad()를 실행함으로서 미분을 통해 얻은 기울기를 0으로 초기화한다.\n",
    "# 파이토치는 기울기를 계속 누적(전에 구한값이 초기화 되지 않고 계속 남아있음) 시키려고 하기 떄문에 초기화시키는 작업이 꼭 필요하다.\n",
    "# 기울기를 초기화 해야함 새로운 가중치 편향에 대하여 새로운 기울기를 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cost.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W,b 에 대한 기울기 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k5MvVqcBr5ST"
   },
   "outputs": [],
   "source": [
    "cost.backward()\n",
    "# cost.backward() 를 호출하면 가중치 W 와 편향 b 에 대한 기울기가 계산된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Chcsaz7psUNc"
   },
   "outputs": [],
   "source": [
    "optimizer.step()\n",
    "# 그 다음에는 우리가 정한 경사하강법 optimizer 의 step 함수를 호출하여, 인수로 들어갔던 W 와 b 에서 리턴되는 변수들의 기울기에 학습률 0.01 을 곱하여 뺴줌으로서 업데이트된다.\n",
    "# W 와 b를 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eiWXKo56soJz"
   },
   "source": [
    "# Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QffZ5nzLxKnd"
   },
   "source": [
    "## optimizer.zero_grad() 가 필요한 이유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EmdFNcP1xO8y"
   },
   "outputs": [],
   "source": [
    "# 파이토치는 미분을 통해 얻은 기울기를 이전에 기울기 값에 누적시키는 특징이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "id": "wVojHnOixUe7",
    "outputId": "e5818536-3428-488a-ab79-abf090a37794"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w 로 미분한 값 : 2.0\n",
      "수식을 w 로 미분한 값 : 4.0\n",
      "수식을 w 로 미분한 값 : 6.0\n",
      "수식을 w 로 미분한 값 : 8.0\n",
      "수식을 w 로 미분한 값 : 10.0\n",
      "수식을 w 로 미분한 값 : 12.0\n",
      "수식을 w 로 미분한 값 : 14.0\n",
      "수식을 w 로 미분한 값 : 16.0\n",
      "수식을 w 로 미분한 값 : 18.0\n",
      "수식을 w 로 미분한 값 : 20.0\n",
      "수식을 w 로 미분한 값 : 22.0\n",
      "수식을 w 로 미분한 값 : 24.0\n",
      "수식을 w 로 미분한 값 : 26.0\n",
      "수식을 w 로 미분한 값 : 28.0\n",
      "수식을 w 로 미분한 값 : 30.0\n",
      "수식을 w 로 미분한 값 : 32.0\n",
      "수식을 w 로 미분한 값 : 34.0\n",
      "수식을 w 로 미분한 값 : 36.0\n",
      "수식을 w 로 미분한 값 : 38.0\n",
      "수식을 w 로 미분한 값 : 40.0\n",
      "수식을 w 로 미분한 값 : 42.0\n"
     ]
    }
   ],
   "source": [
    "W = torch.tensor(2.0,requires_grad=True)\n",
    "\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs + 1):\n",
    "    z = 2*W\n",
    "    z.backward()\n",
    "    print('수식을 w 로 미분한 값 : {}'.format(W.grad)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6FpZXU0jxqXX"
   },
   "outputs": [],
   "source": [
    "# 계속해서 미분값인 2가 누적되고 있다! \n",
    "# 그렇기 떄문에 중간중간 optimizer.zero_grad() 를 통해서 미분값을 계속 0으로 초기화 시켜주어야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "X_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[2],[4],[6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## low level\n",
    "- low level 은 y_pred / W,b 초기화 방법 / cost 의 정의 들을 사용자가 직접 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.SGD([W,b],lr=0.05)\n",
    "\n",
    "# epochs 설정\n",
    "n_epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 W: 0.933, b: 0.400 Cost: 18.666666\n",
      "Epoch  100/2000 W: 1.904, b: 0.217 Cost: 0.006942\n",
      "Epoch  200/2000 W: 1.971, b: 0.065 Cost: 0.000618\n",
      "Epoch  300/2000 W: 1.991, b: 0.019 Cost: 0.000055\n",
      "Epoch  400/2000 W: 1.997, b: 0.006 Cost: 0.000005\n",
      "Epoch  500/2000 W: 1.999, b: 0.002 Cost: 0.000000\n",
      "Epoch  600/2000 W: 2.000, b: 0.001 Cost: 0.000000\n",
      "Epoch  700/2000 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  800/2000 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch  900/2000 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1000/2000 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1100/2000 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1200/2000 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1300/2000 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1400/2000 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1500/2000 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1600/2000 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1700/2000 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1800/2000 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1900/2000 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 2000/2000 W: 2.000, b: 0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs +1 ) :\n",
    "    # y_hat 계산\n",
    "    y_pred = X_train * W + b\n",
    "    # cost 계산\n",
    "    cost = torch.mean((y_pred-y_train)**2)\n",
    "    # cost 를 토대로 update\n",
    "    optimizer.zero_grad() # gradient 를 0 으로 초기화\n",
    "    cost.backward() # W,b 에 대한 gradient 계산\n",
    "    optimizer.step() # update W,b\n",
    "\n",
    "    # 100번마다 기록 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, n_epochs, W.item(), b.item(), cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t4ebkmzt049K"
   },
   "source": [
    "## High level [nn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- high level 에서는 model, cost 등을 nn.funtional 에서 정의된 값을 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V8-3XfhF1Jac"
   },
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(1,1)\n",
    "# Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "# nn.Linear 은 입력의 차원, 출력의 차원을 인수로 받는다.\n",
    "# 모델에는 편리하게도 가중치 W 와 편향 b 가 저장되어있다.\n",
    "# 단순 선형 회귀이므로 입력차원은 1이고 출력차원도 1이다. input_dim=1, output_dim=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "GjJpZkRH1Kug",
    "outputId": "2394c38e-960a-45ce-a4bf-8115121e908f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.0710]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9941], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters())) \n",
    "# 이 값은 model.parameters() 함수를 통해 불러올 수 있다.\n",
    "# 현재는 두 값이 출력되는데, 첫번쨰 값은 w 이고, 두번쨰 값이 b 에 해당된다.\n",
    "# 두 값 모두 현재는 random 초기화가 되어있는것을 볼 수 있다. (0 초기화보다 random 초기화가 성능이 더 좋음)\n",
    "# 두 값 모두 학습의 대상이므로 requires_grad = True 가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sF-v9sUe2aox"
   },
   "outputs": [],
   "source": [
    "# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "id": "KXVfHDfV27s_",
    "outputId": "69a5bb1d-f0e9-4510-95c4-52b069e46c51",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 10.683009\n",
      "Epoch  100/2000 Cost: 0.211294\n",
      "Epoch  200/2000 Cost: 0.130567\n",
      "Epoch  300/2000 Cost: 0.080682\n",
      "Epoch  400/2000 Cost: 0.049857\n",
      "Epoch  500/2000 Cost: 0.030809\n",
      "Epoch  600/2000 Cost: 0.019038\n",
      "Epoch  700/2000 Cost: 0.011764\n",
      "Epoch  800/2000 Cost: 0.007270\n",
      "Epoch  900/2000 Cost: 0.004492\n",
      "Epoch 1000/2000 Cost: 0.002776\n",
      "Epoch 1100/2000 Cost: 0.001715\n",
      "Epoch 1200/2000 Cost: 0.001060\n",
      "Epoch 1300/2000 Cost: 0.000655\n",
      "Epoch 1400/2000 Cost: 0.000405\n",
      "Epoch 1500/2000 Cost: 0.000250\n",
      "Epoch 1600/2000 Cost: 0.000155\n",
      "Epoch 1700/2000 Cost: 0.000096\n",
      "Epoch 1800/2000 Cost: 0.000059\n",
      "Epoch 1900/2000 Cost: 0.000036\n",
      "Epoch 2000/2000 Cost: 0.000023\n"
     ]
    }
   ],
   "source": [
    "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    y_pred = model(X_train) \n",
    "    # cost 계산\n",
    "    cost = torch.nn.functional.mse_loss(y_pred, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward() \n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High level [class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4W9Rlc6A5Tax"
   },
   "outputs": [],
   "source": [
    "class SimpleLinearRegression (torch.nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__() # 자식클래스에서 부모클래스의 내용을 사용하고 싶은 경우 super().부모클래스내용\n",
    "                           # (이떄에는 init 을 사용하여서 부모클래스에서의 init 을 훔쳐서 attribute 형성하는것을 다 훔친다.)\n",
    "        self.linear = torch.nn.Linear(1,1) # attribute 형성, 이때에는 단순선형회귀 이므로 (1,1) 을 형성한다.\n",
    "    def forward (self, x): # 모델이 학습 데이터를 입력받아서 forward 연산을 진행시키는 메서드 \n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ysjtSGrJ5fSY"
   },
   "outputs": [],
   "source": [
    "# 모델을 선언 및 초기화. \n",
    "model = SimpleLinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MiCvl06M7OR6"
   },
   "outputs": [],
   "source": [
    "# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "id": "1DJ_lznx7ydY",
    "outputId": "8846c30f-3095-4f17-ea76-68409afa28a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 16.180552\n",
      "Epoch  100/2000 Cost: 0.223619\n",
      "Epoch  200/2000 Cost: 0.138183\n",
      "Epoch  300/2000 Cost: 0.085389\n",
      "Epoch  400/2000 Cost: 0.052765\n",
      "Epoch  500/2000 Cost: 0.032606\n",
      "Epoch  600/2000 Cost: 0.020148\n",
      "Epoch  700/2000 Cost: 0.012450\n",
      "Epoch  800/2000 Cost: 0.007694\n",
      "Epoch  900/2000 Cost: 0.004754\n",
      "Epoch 1000/2000 Cost: 0.002938\n",
      "Epoch 1100/2000 Cost: 0.001815\n",
      "Epoch 1200/2000 Cost: 0.001122\n",
      "Epoch 1300/2000 Cost: 0.000693\n",
      "Epoch 1400/2000 Cost: 0.000428\n",
      "Epoch 1500/2000 Cost: 0.000265\n",
      "Epoch 1600/2000 Cost: 0.000164\n",
      "Epoch 1700/2000 Cost: 0.000101\n",
      "Epoch 1800/2000 Cost: 0.000062\n",
      "Epoch 1900/2000 Cost: 0.000039\n",
      "Epoch 2000/2000 Cost: 0.000024\n"
     ]
    }
   ],
   "source": [
    "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    prediction = model(X_train)\n",
    "    cost = torch.nn.functional.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward() \n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilinear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9_bGSn6bnlM"
   },
   "outputs": [],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## low level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W1c9s9uibso2"
   },
   "outputs": [],
   "source": [
    "W = torch.zeros((3, 1), requires_grad=True) # x 가 5*3 matrix 이므로/ 가중치 W 는 3*1 matrix\n",
    "b = torch.zeros(1, requires_grad=True) # b 는 그냥 마지막에 계산되는 곁다리 이므로(WX+b) 스칼라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0JG9NHQ_ciQX"
   },
   "outputs": [],
   "source": [
    "# optimizer 설정\n",
    "optimizer = torch.optim.SGD([W, b], lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "id": "al0eTWIgb9S-",
    "outputId": "2c286843-9a30-4594-b2f0-89dd05092a84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 y_pred: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 y_pred: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.520508\n",
      "Epoch    2/20 y_pred: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712646\n",
      "Epoch    3/20 y_pred: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040527\n",
      "Epoch    4/20 y_pred: tensor([137.7968, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936005\n",
      "Epoch    5/20 y_pred: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371017\n",
      "Epoch    6/20 y_pred: tensor([148.1035, 178.0144, 175.3980, 191.0042, 135.7812]) Cost: 29.758139\n",
      "Epoch    7/20 y_pred: tensor([150.1744, 180.5042, 177.8508, 193.6753, 137.6805]) Cost: 10.445305\n",
      "Epoch    8/20 y_pred: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391228\n",
      "Epoch    9/20 y_pred: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493135\n",
      "Epoch   10/20 y_pred: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688\n",
      "Epoch   11/20 y_pred: tensor([152.5485, 183.3610, 180.6640, 196.7389, 139.8602]) Cost: 1.710541\n",
      "Epoch   12/20 y_pred: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651413\n",
      "Epoch   13/20 y_pred: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632387\n",
      "Epoch   14/20 y_pred: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625923\n",
      "Epoch   15/20 y_pred: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623412\n",
      "Epoch   16/20 y_pred: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622141\n",
      "Epoch   17/20 y_pred: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621253\n",
      "Epoch   18/20 y_pred: tensor([152.7999, 183.6688, 180.9644, 197.0662, 140.0963]) Cost: 1.620500\n",
      "Epoch   19/20 y_pred: tensor([152.8014, 183.6715, 180.9666, 197.0686, 140.0985]) Cost: 1.619770\n",
      "Epoch   20/20 y_pred: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.1000]) Cost: 1.619033\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # y_pred 예측하는 모델 형성\n",
    "    y_pred = x_train.matmul(W) + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((y_pred - y_train) ** 2)\n",
    "\n",
    "    # cost로  w update\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    print('Epoch {:4d}/{} y_pred: {} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, y_pred.squeeze().detach(), cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r3tU4jJQbkq-"
   },
   "outputs": [],
   "source": [
    "# y_pred 모델형성\n",
    "y_pred = x_train.matmul(W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DyPRTUiR3Dsm"
   },
   "source": [
    "## High level [nn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EeCrXqnB4Uyr"
   },
   "outputs": [],
   "source": [
    "# 모델을 선언 및 초기화. 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
    "model = torch.nn.Linear(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "MabUPn7d4WQ2",
    "outputId": "7e1de5d8-89c6-4137-e37c-00ba9a20209a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.3789, 0.1685, 0.0628]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1498], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))\n",
    "# 첫 3개의 출력이 w이다.\n",
    "# 두번쨰 출력되는것이 b 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "77D4o_W34ZCh"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "id": "w5juzpkD4e2s",
    "outputId": "87d5b164-23ae-4912-c61d-ecf74bc740b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 14458.637695\n",
      "Epoch  100/2000 Cost: 0.716153\n",
      "Epoch  200/2000 Cost: 0.689292\n",
      "Epoch  300/2000 Cost: 0.663840\n",
      "Epoch  400/2000 Cost: 0.639716\n",
      "Epoch  500/2000 Cost: 0.616854\n",
      "Epoch  600/2000 Cost: 0.595202\n",
      "Epoch  700/2000 Cost: 0.574659\n",
      "Epoch  800/2000 Cost: 0.555219\n",
      "Epoch  900/2000 Cost: 0.536784\n",
      "Epoch 1000/2000 Cost: 0.519317\n",
      "Epoch 1100/2000 Cost: 0.502749\n",
      "Epoch 1200/2000 Cost: 0.487065\n",
      "Epoch 1300/2000 Cost: 0.472183\n",
      "Epoch 1400/2000 Cost: 0.458081\n",
      "Epoch 1500/2000 Cost: 0.444715\n",
      "Epoch 1600/2000 Cost: 0.432049\n",
      "Epoch 1700/2000 Cost: 0.420045\n",
      "Epoch 1800/2000 Cost: 0.408664\n",
      "Epoch 1900/2000 Cost: 0.397871\n",
      "Epoch 2000/2000 Cost: 0.387638\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    # model(x_train)은 model.forward(x_train)와 동일함.\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.nn.functional.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward()\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Ydxfhxyv41Om",
    "outputId": "76c1fd17-5220-4e07-d1d2-d804a68dcc29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.9635]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [73, 80, 75]를 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) \n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "cbICCOQY43Nn",
    "outputId": "0b04632f-80ea-4437-9894-ad8a9da3b31b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.9009, 0.5724, 0.5406]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1426], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 학습 후의 최적의 w 와 b\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hx1eCcJm71Aa"
   },
   "source": [
    "## High level [class]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EJ01jZpO8m8D"
   },
   "outputs": [],
   "source": [
    "class MultivariateLinearRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LYBuVQp28qZU"
   },
   "outputs": [],
   "source": [
    "model = MultivariateLinearRegression()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "id": "ApQkwH_d8xPJ",
    "outputId": "abef8bfc-1c2b-41db-dd22-c155b3f1c2d4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 40985.058594\n",
      "Epoch  100/2000 Cost: 3.846339\n",
      "Epoch  200/2000 Cost: 3.691392\n",
      "Epoch  300/2000 Cost: 3.544453\n",
      "Epoch  400/2000 Cost: 3.405098\n",
      "Epoch  500/2000 Cost: 3.272837\n",
      "Epoch  600/2000 Cost: 3.147396\n",
      "Epoch  700/2000 Cost: 3.028405\n",
      "Epoch  800/2000 Cost: 2.915454\n",
      "Epoch  900/2000 Cost: 2.808306\n",
      "Epoch 1000/2000 Cost: 2.706609\n",
      "Epoch 1100/2000 Cost: 2.610105\n",
      "Epoch 1200/2000 Cost: 2.518515\n",
      "Epoch 1300/2000 Cost: 2.431545\n",
      "Epoch 1400/2000 Cost: 2.348994\n",
      "Epoch 1500/2000 Cost: 2.270626\n",
      "Epoch 1600/2000 Cost: 2.196196\n",
      "Epoch 1700/2000 Cost: 2.125513\n",
      "Epoch 1800/2000 Cost: 2.058384\n",
      "Epoch 1900/2000 Cost: 1.994623\n",
      "Epoch 2000/2000 Cost: 1.934033\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    y_pred = model(x_train)\n",
    "    cost = torch.nn.functional.mse_loss(y_pred, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ZbOkc4DvBl9d",
    "outputId": "312fad87-395f-4cec-e7bc-0f623e3cb91e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[152.2236]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [73, 80, 75]를 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) \n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nez0vUV_FAsX"
   },
   "source": [
    "앞에서 데이터셋을 다룬것처럼 파이토치에서는 데이터셋을 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 torch.utils.data.Dataset과 torch.utils.data.DataLoader를 제공합니다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다. 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "31kJvoC5FAo7"
   },
   "source": [
    "## 실전방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HEOALjfQFsGw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[73, 80, 75],\n",
    "          [93, 88, 93],\n",
    "          [89, 91, 90],\n",
    "          [96, 98, 100],\n",
    "          [73, 66, 70]]\n",
    "y_train= [[152], [185], [180], [196], [142]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jBFrk62lFuaE"
   },
   "outputs": [],
   "source": [
    "# 데이터 set 정의\n",
    "class CustomDataset(Dataset): \n",
    "    def __init__(self):   #데이터셋의 전처리를 해주는 부분\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "        \n",
    "    def __len__(self):   #데이터셋 길이(총 샘플의 수) 를 도출하는 매직 메서드를 정의한다.\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):   # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴(#데이터셋에서 특정 1개의 샘플을 가져오는 함수)하는 메서드를 정의한다.\n",
    "        x = torch.FloatTensor(self.x_data[idx]) # dataset[i]을 했을 때 i번째 샘플을 가져오도록 하는 인덱싱을 위한 get_item\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZHJJiEWqF37O"
   },
   "outputs": [],
   "source": [
    "dataset = CustomDataset() # 데이터셋을 원래 형태에서 전처리->pytorch 데이터 형태로 바로 변환이 가능하다.\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 정의\n",
    "class MultivariateLinearRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q6WTvWA3F61F"
   },
   "outputs": [],
   "source": [
    "model = MultivariateLinearRegression()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CWCtfAsmF8OF",
    "outputId": "be9d068c-13ce-4634-fe5c-68e146896ae9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/3 Batch 1/3 Cost: 54475.851562\n",
      "Epoch    0/3 Batch 2/3 Cost: 38651.292969\n",
      "Epoch    0/3 Batch 3/3 Cost: 11191.161133\n",
      "Epoch    1/3 Batch 1/3 Cost: 1890.706543\n",
      "Epoch    1/3 Batch 2/3 Cost: 404.244720\n",
      "Epoch    1/3 Batch 3/3 Cost: 99.452049\n",
      "Epoch    2/3 Batch 1/3 Cost: 46.519470\n",
      "Epoch    2/3 Batch 2/3 Cost: 22.476124\n",
      "Epoch    2/3 Batch 3/3 Cost: 13.367160\n",
      "Epoch    3/3 Batch 1/3 Cost: 1.953240\n",
      "Epoch    3/3 Batch 2/3 Cost: 0.134132\n",
      "Epoch    3/3 Batch 3/3 Cost: 6.712824\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 3\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "    # print(batch_idx)\n",
    "    # print(samples)\n",
    "        x_train, y_train = samples\n",
    "        y_pred = model(x_train)\n",
    "        cost = F.mse_loss(y_pred, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "        cost.item()\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hvN72lYhF9tT",
    "outputId": "dd13bac3-afef-4936-adee-616457e69fa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[152.3978]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [73, 80, 75]를 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) \n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhO1o0O_mnJt"
   },
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "kCFhpwrRmL_V",
    "outputId": "b5fb8762-9983-4244-a3fd-ebd7977fddaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d6da38a790>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1GOAYl3CmkY0"
   },
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "QJH2YpJpmlyw",
    "outputId": "d1ba12c0-2703-41a8-e10b-00cbc379f9d7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8Qwsx23v7a-"
   },
   "source": [
    "## low level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3OEF6PR6mykS"
   },
   "outputs": [],
   "source": [
    "W = torch.zeros((2, 1), requires_grad=True) # 크기는 2 x 1\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qiCc4BgOm04G"
   },
   "outputs": [],
   "source": [
    "# 둘다 같은 식이다. torch 에서는 simgmoid 를 지원해준다.\n",
    "y_pred = 1 / (1 + torch.exp(-(x_train.matmul(W) + b))) # y_hat\n",
    "y_pred = torch.sigmoid(x_train.matmul(W) + b) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "rIGQrDyxnJ5N",
    "outputId": "61ca637f-84b9-4142-cf41-313a5e7be6c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0013],\n",
      "        [0.0581],\n",
      "        [0.0768],\n",
      "        [0.0802],\n",
      "        [0.0059],\n",
      "        [0.0014]], grad_fn=<NegBackward>)\n",
      "tensor(0.0373, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0373, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss(개별) 정의 , cost 정의\n",
    "loss = -(y_train * torch.log(y_pred) + \n",
    "           (1 - y_train) * torch.log(1 - y_pred))\n",
    "print(loss)\n",
    "cost = loss.mean()\n",
    "print(cost)\n",
    "\n",
    "F.binary_cross_entropy(y_pred, y_train) # 로 바로 계산할수도있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "qKFOdmoyn170",
    "outputId": "a11e3f24-4f4d-47bb-9b5f-ee74dde7bc41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/500 Cost \"0.693147\n",
      "Epoch  100/500 Cost \"0.134722\n",
      "Epoch  200/500 Cost \"0.080643\n",
      "Epoch  300/500 Cost \"0.057900\n",
      "Epoch  400/500 Cost \"0.045300\n",
      "Epoch  500/500 Cost \"0.037261\n"
     ]
    }
   ],
   "source": [
    "# 모델 변수설정\n",
    "W = torch.zeros((2,1),requires_grad=True)\n",
    "b = torch.zeros(1,requires_grad=True) \n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W,b], lr=1)\n",
    "\n",
    "# epoch 설정\n",
    "nb_epochs = 500\n",
    "for epoch in range(nb_epochs+1) :\n",
    "    y_pred = torch.sigmoid(x_train.matmul(W) + b)\n",
    "    loss = -(y_train * torch.log(y_pred) + \n",
    "           (1 - y_train) * torch.log(1 - y_pred))\n",
    "    cost = loss.mean()\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()  \n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {0:4d}/{1} Cost \"{2:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "wF9UmjQ7r3WR",
    "outputId": "084add27-48af-4a3b-878d-6c4b07fff1dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.6517],\n",
      "        [1.1815]], requires_grad=True)\n",
      "tensor([-11.6667], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측\n",
    "# 현재 W,b 는 requires_grad 가 변하는것을 True 로 두었으므로, 예측값이 update 되어있는 상태이다.\n",
    "print(W)\n",
    "print(b)\n",
    "predict = torch.sigmoid(x_train.matmul(W) + b)\n",
    "predict > torch.FloatTensor([0.5])  #  예측 나름 잘하는듯?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFFq8vWDsk91"
   },
   "source": [
    "## High level [nn]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9aCI6Pb8tWc0"
   },
   "source": [
    "nn.Sequential()은 nn.Module 층을 차례로 쌓을 수 있도록 합니다. 뒤에서 이를 이용해서 인공 신경망을 구현하게 되므로 기억하고 있으면 좋습니다. 조금 쉽게 말해서 nn.Sequential()은 Wx+b와 같은 수식과 시그모이드 함수 등과 같은 여러 함수들을 연결해주는 역할을 합니다. 이를 이용해서 로지스틱 회귀를 구현해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a3AFe5aWtcaG"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "   nn.Linear(2, 1), # input_dim = 2, output_dim = 1\n",
    "                    # Linear(2,1) 로 선언이 되어있으면, W,b 가 requres_grad = True 가 되어있는 상태로 초기화 되어서 자동으로 저장된다.\n",
    "   nn.Sigmoid() ) # 출력은 시그모이드 함수를 거친다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "enMlKp99tnEs",
    "outputId": "567ab600-8f9d-4c8b-eb8f-6db8e5dc22d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.4271, -0.1185]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3050], requires_grad=True)]\n",
      "tensor([[0.2751],\n",
      "        [0.1802],\n",
      "        [0.1538],\n",
      "        [0.0856],\n",
      "        [0.0575],\n",
      "        [0.0429]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters())) # 모델이 가지고 있는 파라미터 (W,b 출력) (랜덤초기화 되어있는상태)\n",
    "print(model(x_train)) # 예측값 .사실 가중치 초기화만 되어있고 학습은 안된상태라 의미는 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "Xm2j61QpuPuk",
    "outputId": "017337d9-8959-4a13-832e-a310f0f0f191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/500 Cost: 1.524874 Accuracy 50.00\n",
      "Epoch  100/500 Cost: 0.132412 Accuracy 100.00\n",
      "Epoch  200/500 Cost: 0.079831 Accuracy 100.00\n",
      "Epoch  300/500 Cost: 0.057487 Accuracy 100.00\n",
      "Epoch  400/500 Cost: 0.045049 Accuracy 100.00\n",
      "Epoch  500/500 Cost: 0.037092 Accuracy 100.00\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 500\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = model(x_train)\n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        y_pred = (hypothesis >= torch.FloatTensor([0.5])) # 예측값이 0.5를 넘으면 True로 간주\n",
    "        correct_prediction = (y_pred.float() == y_train) # 실제값과 일치하는 경우만 True로 간주\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
    "                                                                             # torch 에 하나의 값만 존재한다면, item 을 통해 값을 꺼낼 수 있다.\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}'.format( # 각 에포크마다 정확도를 출력\n",
    "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "racxX-AaumFS"
   },
   "source": [
    "## High level [class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9utfJWrGwsWE"
   },
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # nn.Module 의 init 을 실행한 후 거기에다가 모델 추가\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x): #  모델이 학습데이터를 입력받아서 forward 연산을 진행시키는 메서드\n",
    "        return self.sigmoid(self.linear(x)) # forward 부분의 함수연산\n",
    "model = BinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "lP2FoXXiwtYp",
    "outputId": "8e8929f6-a8db-4f3c-eb7c-f678da6f7a0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.886008 Accuracy 16.67%\n",
      "Epoch  100/1000 Cost: 0.138322 Accuracy 100.00%\n",
      "Epoch  200/1000 Cost: 0.081886 Accuracy 100.00%\n",
      "Epoch  300/1000 Cost: 0.058527 Accuracy 100.00%\n",
      "Epoch  400/1000 Cost: 0.045678 Accuracy 100.00%\n",
      "Epoch  500/1000 Cost: 0.037515 Accuracy 100.00%\n",
      "Epoch  600/1000 Cost: 0.031855 Accuracy 100.00%\n",
      "Epoch  700/1000 Cost: 0.027693 Accuracy 100.00%\n",
      "Epoch  800/1000 Cost: 0.024502 Accuracy 100.00%\n",
      "Epoch  900/1000 Cost: 0.021975 Accuracy 100.00%\n",
      "Epoch 1000/1000 Cost: 0.019923 Accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    hypothesis = model(x_train)\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n",
    "        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\n",
    "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pr70cBCSCfdW"
   },
   "source": [
    "# Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pR7M16dCm7f"
   },
   "source": [
    "## cost 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M3qWRd771k9W"
   },
   "outputs": [],
   "source": [
    "z = torch.FloatTensor([1, 2, 3]) # 이 텐서를 소프트맥스 함수의 입력으로 사용하고, 그 결과를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "RD1avrov1w7_",
    "outputId": "073cfba2-38a3-4b4d-e36c-cc0eae1a236a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "y_pred = F.softmax(z, dim=0) # row 로 softmax 를 적용한다는 뜻.\n",
    "print(y_pred)\n",
    "# 3개의 원소의 값이 0과 1사이의 값을 가지는 벡터로 변환된 것을 볼 수 있습니다. 이 원소들의 값의 합이 1인지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "4U1UmvpW3J89",
    "outputId": "263cd37f-b107-4fd2-90bd-3157ac115145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor([[0.2128, 0.2974, 0.2574, 0.1173, 0.1150],\n",
      "        [0.1486, 0.2933, 0.1740, 0.2342, 0.1499],\n",
      "        [0.2880, 0.1427, 0.2565, 0.1774, 0.1355]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.sum())\n",
    "# 총 원소의 값의 합은 1입니다. 이번에는 비용 함수를 직접 구현해보겠습니다. 우선 임의의 3 × 5 행렬의 크기를 가진 텐서를 만듭니다.\n",
    "\n",
    "z = torch.rand(3, 5, requires_grad=True)\n",
    "# 이제 이 텐서에 대해서 소프트맥스 함수를 적용합니다.\n",
    "#  단, 각 샘플에 대해서 소프트맥스 함수를 적용하여야 하므로 두번째 차원(col)에 대해서 소프트맥스 함수를 적용한다는 의미에서 dim=1을 써줍니다.\n",
    "\n",
    "y_pred = F.softmax(z, dim=1)\n",
    "# col 에 대해서 확률값이 나올수 있게 (연산의 방향을 ->) softmax 를 적용한다는 의미이다.\n",
    "\n",
    "print(y_pred)\n",
    "# 이제 각 행의 원소들의 합은 1이 되는 텐서로 변환되었습니다. 소프트맥스 함수의 출력값은 결국 예측값입니다. \n",
    "# 즉, 위 텐서는 3개의 샘플에 대해서 5개의 클래스 중 어떤 클래스가 정답인지를 예측한 결과입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TRy1xW5jArm2"
   },
   "source": [
    "### one hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "1h7UKzJN3fV_",
    "outputId": "0b0fd04a-ebaf-4abf-d328-69d0d491bd93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "#이제 각 샘플에 대해서 임의의 레이블을 만듭니다.\n",
    "y = torch.randint(5, (3,)).long() #0~5 의 값을 (3,) 형태의 vector 로 random 하게 형성\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "HDZWd2S--rZC",
    "outputId": "f56f0f64-8b24-40af-8ec1-d84192ba7e5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [3],\n",
      "        [4]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 이제 각 레이블에 대해서 원-핫 인코딩을 수행합니다.\n",
    "# 모든 원소가 0의 값을 가진 3 × 5 텐서 생성\n",
    "y_one_hot = torch.zeros_like(y_pred)  \n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1) # unsqueeze 는 인수로 받은 위치에 새로운 차원을 삽입한다.\n",
    "# col 을 기준으로 1을 할당하겠다는 의미.\n",
    "# .scatter_ (dim ,index, 값 ) -> dim = 기준이 되는 dim, 0 이면 제일 처음(row) 기준, 1 이면 안쪽(col) 기준\n",
    "\n",
    "# 위의 연산에서 어떻게 원-핫 인코딩이 수행되었는지 보겠습니다. \n",
    "# 우선, torch.zeros_like(hypothesis)를 통해 모든 원소가 0의 값을 가진 3 × 5 텐서를 만듭니다. \n",
    "# 그리고 이 텐서는 y_one_hot에 저장이 된 상태입니다.\n",
    "# 두번째 줄을 해석해봅시다. y.unsqueeze(1)를 하면 (3,)의 크기를 가졌던 y 텐서는 (3 × 1) 텐서가 됩니다.\n",
    "# 즉, 다시 말해서 y.unsqueeze(1)의 결과는 아래와 같습니다. \n",
    "print(y.unsqueeze(1))  #vector 에서 col 이 하나인 matrix 가 나온다.\n",
    "\n",
    "# scatter_(dim, index, src) \n",
    "# 그리고 scatter의 첫번째 인자로 dim=1(col) 에 대해서 수행하라고 알려주고, \n",
    "# 세번째 인자에 숫자 1을 넣어주므로서 두번째 인자(index)인 y_unsqeeze(1)이 알려주는 위치에 숫자 1을 넣도록 합니다. \n",
    "# 연산 뒤에 _를 붙이면 In-place Operation (덮어쓰기 연산)임을 배운 바 있습니다.\n",
    "#  이에 따라서 y_one_hot의 최종 결과는 결국 아래와 같습니다.\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "cfPBSiBS_IYn",
    "outputId": "1e67f8ae-740a-4172-9ab3-f727320326b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "tensor([[0.2128, 0.2974, 0.2574, 0.1173, 0.1150],\n",
      "        [0.1486, 0.2933, 0.1740, 0.2342, 0.1499],\n",
      "        [0.2880, 0.1427, 0.2565, 0.1774, 0.1355]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(1.6659, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 이제 비용 함수 연산을 위한 재료들을 전부 손질했습니다. 소프트맥스 회귀의 비용 함수를 써 봅시다.\n",
    "print(y_one_hot)\n",
    "print(y_pred)\n",
    "cost = (y_one_hot * -torch.log(y_pred)).sum(axis=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DQRgarRu-xRC"
   },
   "source": [
    "## cost 구현 (High)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "hn2kqmhJC29E",
    "outputId": "dcdc841f-4dec-4b45-8aed-6fea2a6638fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2797, -1.8830, -1.9087, -1.5010, -1.6166],\n",
      "        [-1.3353, -1.9185, -2.0734, -1.2651, -1.7031],\n",
      "        [-1.4088, -1.5293, -2.0650, -1.5896, -1.5700]], grad_fn=<LogBackward>)\n",
      "tensor([[-1.2797, -1.8830, -1.9087, -1.5010, -1.6166],\n",
      "        [-1.3353, -1.9185, -2.0734, -1.2651, -1.7031],\n",
      "        [-1.4088, -1.5293, -2.0650, -1.5896, -1.5700]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 이제 소프트맥스의 비용 함수를 좀 더 하이-레벨로 구현하는 방법에 대해서 알아봅시다.\n",
    "\n",
    "#1. F.softmax() + torch.log() = F.log_softmax()\n",
    "# 앞서 소프트맥스 함수의 결과에 로그를 씌울 때는 다음과 같이 소프트맥스 함수의 출력값을 로그 함수의 입력으로 사용했습니다.\n",
    "\n",
    "# Low level\n",
    "print( torch.log(F.softmax(z, dim=1)) )\n",
    "# 그런데 파이토치에서는 두 개의 함수를 결합한 F.log_softmax()라는 도구를 제공합니다.\n",
    "\n",
    "# High level\n",
    "print( F.log_softmax(z, dim=1) )\n",
    "\n",
    "#두 출력 결과가 동일한 것을 볼 수 있습니다. 이제 비용 함수를 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "aTODm5hWENoe",
    "outputId": "9c6090e5-6472-4328-9ff6-1853a36d1de1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8798, grad_fn=<MeanBackward0>)\n",
      "tensor(1.8798, grad_fn=<MeanBackward0>)\n",
      "tensor(1.8798, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8798, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 2. F.log_softmax() + F.nll_loss() = F.cross_entropy()\n",
    "# 앞서 로우-레벨로 구현한 비용 함수는 다음과 같았습니다.\n",
    "\n",
    "# Low level\n",
    "# 첫번째 수식\n",
    "print( (y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean() )\n",
    "\n",
    "# 그런데 위의 수식에서 torch.log(F.softmax(z, dim=1))를 방금 배운 F.log_softmax()로 대체할 수 있습니다.\n",
    "\n",
    "# 두번째 수식\n",
    "print( (y_one_hot * - F.log_softmax(z, dim=1)).sum(dim=1).mean() )\n",
    "# 이를 더 간단하게 하면 다음과 같습니다. F.nll_loss()를 사용할 때는 원-핫 벡터를 넣을 필요없이 바로 실제값을 인자로 사용합니다.\n",
    "\n",
    "# High level\n",
    "# 세번째 수식\n",
    "print( F.nll_loss(F.log_softmax(z, dim=1), y) )\n",
    "# 여기서 nll이란 Negative Log Likelihood의 약자입니다. 위에서 nll_loss는 F.log_softmax()를 수행한 후에 남은 수식들을 수행합니다. 이를 더 간단하게 하면 다음과 같이 사용할 수 있습니다. F.cross_entropy()는 F.log_softmax()와 F.nll_loss()를 포함하고 있습니다.\n",
    "\n",
    "# 네번째 수식\n",
    "print( F.cross_entropy(z, y) )\n",
    "# F.cross_entropy는 비용 함수에 소프트맥스 함수까지 포함하고 있음을 기억하고 있어야 구현 시 혼동하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mz56UxfsFZ5W"
   },
   "source": [
    "# Softmax Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7EKEW5SdFXtE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)  # lable 이라 intger 형태일것이므로 LongTensor(정수텐서)(64bit integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4AJtap_oF-nA"
   },
   "source": [
    "## low level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "B5L9fJcoGQ99",
    "outputId": "b4654d32-0c3a-4570-a2e8-ed54b7dc4c3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 3])\n"
     ]
    }
   ],
   "source": [
    "# y ont hoy encoding 하기\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "# x_train의 크기는 8 × 4이며, y_train의 크기는 8 × 1입니다. \n",
    "# 그런데 최종 사용할 레이블은 y_train에서 원-핫 인코딩을 한 결과이어야 합니다. \n",
    "# 클래스의 개수는 3개이므로 y_train에 원-핫 인코딩한 결과는 8 × 3의 개수를 가져야 합니다.\n",
    "\n",
    "y_one_hot = torch.zeros(8, 3)\n",
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "print(y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "imI4_0aIGYud",
    "outputId": "5d49faff-83ed-46a1-c8b4-e57392a39bc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.761050\n",
      "Epoch  200/1000 Cost: 0.689991\n",
      "Epoch  300/1000 Cost: 0.643229\n",
      "Epoch  400/1000 Cost: 0.604117\n",
      "Epoch  500/1000 Cost: 0.568255\n",
      "Epoch  600/1000 Cost: 0.533922\n",
      "Epoch  700/1000 Cost: 0.500291\n",
      "Epoch  800/1000 Cost: 0.466908\n",
      "Epoch  900/1000 Cost: 0.433507\n",
      "Epoch 1000/1000 Cost: 0.399962\n"
     ]
    }
   ],
   "source": [
    "# y_train에서 원-핫 인코딩을 한 결과인 y_one_hot의 크기는 8 × 3입니다. 즉, W 행렬의 크기는 4 × 3이어야 합니다.\n",
    "# W와 b를 선언하고, 옵티마이저로는 경사 하강법을 사용합니다. 그리고 학습률은 0.1로 설정합니다.\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # 가설\n",
    "    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1) \n",
    "\n",
    "    # 비용 함수\n",
    "    # F.softmax()와 torch.log()를 사용하여 가설과 비용 함수를 정의하고, 총 1,000번의 에포크를 수행합니다.\n",
    "    cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Zn9IGRtGuq8"
   },
   "source": [
    "## High level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "HC5fjyOUGtVB",
    "outputId": "114d773e-714f-4e2e-bf6c-a40e33e30a6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.761050\n",
      "Epoch  200/1000 Cost: 0.689991\n",
      "Epoch  300/1000 Cost: 0.643229\n",
      "Epoch  400/1000 Cost: 0.604117\n",
      "Epoch  500/1000 Cost: 0.568255\n",
      "Epoch  600/1000 Cost: 0.533922\n",
      "Epoch  700/1000 Cost: 0.500291\n",
      "Epoch  800/1000 Cost: 0.466908\n",
      "Epoch  900/1000 Cost: 0.433507\n",
      "Epoch 1000/1000 Cost: 0.399962\n"
     ]
    }
   ],
   "source": [
    "# 이제는 F.cross_entropy()를 사용하여 비용 함수를 구현해보겠습니다. \n",
    "# 주의할 점은 F.cross_entropy()는 그 자체로 소프트맥스 함수를 포함하고 있으므로 가설에서는 소프트맥스 함수를 사용할 필요가 없습니다.\n",
    "\n",
    "# 위와 동일한 x_train과 y_train을 사용합니다.\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    z = x_train.matmul(W) + b\n",
    "    cost = F.cross_entropy(z, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))\n",
    "\n",
    "# 예측시 사용할 함수.\n",
    "hypothesis = F.softmax(x_train.matmul(W) + b, dim=1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XzbZqU57G6oV"
   },
   "source": [
    "## nn.Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "hIYYZUyKG78U",
    "outputId": "2ee2c5a3-6076-4774-f15d-25516a944301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.616785\n",
      "Epoch  100/1000 Cost: 0.658891\n",
      "Epoch  200/1000 Cost: 0.573444\n",
      "Epoch  300/1000 Cost: 0.518151\n",
      "Epoch  400/1000 Cost: 0.473265\n",
      "Epoch  500/1000 Cost: 0.433516\n",
      "Epoch  600/1000 Cost: 0.396563\n",
      "Epoch  700/1000 Cost: 0.360914\n",
      "Epoch  800/1000 Cost: 0.325392\n",
      "Epoch  900/1000 Cost: 0.289178\n",
      "Epoch 1000/1000 Cost: 0.254148\n"
     ]
    }
   ],
   "source": [
    "# 이번에는 nn.Module로 소프트맥스 회귀를 구현해봅시다.\n",
    "#  선형 회귀에서 구현에 사용했던 nn.Linear()를 사용합니다.\n",
    "#  output_dim이 1이었던 선형 회귀때와 달리 output_dim은 이제 클래스의 개수여야 합니다.\n",
    "\n",
    "# 모델을 선언 및 초기화. 4개의 특성을 가지고 3개의 클래스로 분류. input_dim=4, output_dim=3.\n",
    "model = nn.Linear(4, 3)\n",
    "\n",
    "# 아래에서 F.cross_entropy()를 사용할 것이므로 따로 소프트맥스 함수를 가설에 정의하지 않습니다.\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b_gaOki5HcD5"
   },
   "source": [
    "## Class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "yrp4p72LHdxo",
    "outputId": "9cefa94b-1d3c-4fd4-c044-c55f68ee9989",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 2.637636\n",
      "Epoch  100/1000 Cost: 0.647903\n",
      "Epoch  200/1000 Cost: 0.564643\n",
      "Epoch  300/1000 Cost: 0.511043\n",
      "Epoch  400/1000 Cost: 0.467249\n",
      "Epoch  500/1000 Cost: 0.428281\n",
      "Epoch  600/1000 Cost: 0.391924\n",
      "Epoch  700/1000 Cost: 0.356742\n",
      "Epoch  800/1000 Cost: 0.321577\n",
      "Epoch  900/1000 Cost: 0.285617\n",
      "Epoch 1000/1000 Cost: 0.250818\n"
     ]
    }
   ],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3) # Output이 3!\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "model = SoftmaxClassifierModel()\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Jz7TyuEI1Js"
   },
   "source": [
    "## GPU 연산 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vtPqmuE1JDpM"
   },
   "source": [
    "구글의 Colab에서 '런타임 > 런타임 유형 변경 > 하드웨어 가속기 > GPU'를 선택하면 USE_CUDA의 값이 True가 되면서 '다음 기기로 학습합니다: cuda'라는 출력이 나옵니다. 즉, GPU로 연산하겠다는 의미입니다. 반면에 '하드웨어 가속기 > None'을 선택하면 USE_CUDA의 값이 False가 되면서 '다음 기기로 학습합니다: cpu'라는 출력이 나옵니다. 즉, CPU로 연산하겠다는 의미입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "D_5bTI0_Ilqs",
    "outputId": "2010e0e0-0533-43c9-f3f5-7bccc37c11dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 기기로 학습합니다: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available() # GPU를 사용가능하면 True, 아니라면 False를 리턴\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") # GPU 사용 가능하면 사용하고 아니면 CPU 사용\n",
    "print(\"다음 기기로 학습합니다:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9djTaFOJMPo"
   },
   "source": [
    "## Random seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3icPf4pIv_H"
   },
   "outputs": [],
   "source": [
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EylCnG7zKddQ"
   },
   "source": [
    "## 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399,
     "referenced_widgets": [
      "720007ff940f44948480bdf66563c44c",
      "f50113df95b040eebe3dad3bd3c870a2",
      "95a62b4511f94cfb9ac0e8f9b982bd6c",
      "c74392d9c53b4f50ab1f8a96e35e7216",
      "b0eece2dc5fb4f68853f8b0daf6c4afa",
      "6d04fead96e44039a670bd65da4ab83d",
      "45f6f42c4e924dbe8cc71941083ebc8e",
      "6130e292804a443f87effbc689a20efa",
      "c23d0fe691874585addb181dc7d48847",
      "4e40cb232b9542f0a21dd24b1e39ec40",
      "21851cf2978e45398a4048ed89a57ef7",
      "19de2aad8a9c45329214bee1275ad029",
      "683f5f06ba1f401386942ae5c23a0041",
      "79e4c38f5e494beaab6654c73a752ad4",
      "0458aa7c092d446eac01a3702aab1e18",
      "5b46a3db9fdd4edbb63b6efe7bb432b9",
      "de1c68605e6c40ecb220df48aae0c83d",
      "bdc68ddea4534d2cbf9906f518585b53",
      "c93aeac5062a4f2984ac37d23f335fe0",
      "db7796c82cf04139b77f6b4f91b89595",
      "f36212938ead480c9c29f4411ca97b42",
      "1e047bf05d7c408c8129db83d072e5cb",
      "5add188d991f4a1bab483369e47b0444",
      "11aa72ba8e0545f38cae342457131c73",
      "84e90558b26f4f6c802b0c7c61dd02c0",
      "580d3814ecbc48d3af327a3326e01d80",
      "976dffb432f24d26a1c8c3015350bfac",
      "328e5aaf11eb482aa4ab1344c1f7fdc9",
      "21683aee8d234cf3912d17bcce4d9eb4",
      "05bbdeef93f246f1b0a28e9a6a7827a1",
      "626f142c5a284d15aff6c64ad2f09dfe",
      "78632155d30741b2ade4c43abf5497ba"
     ]
    },
    "colab_type": "code",
    "id": "jGFpwQoFJR0j",
    "outputId": "793c5895-f28d-4c85-c73d-4ea45f3c34b8"
   },
   "outputs": [],
   "source": [
    "# MNIST dataset 읽어오기\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QqUVeQd7KhOM"
   },
   "source": [
    "## 데이터 loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yf_vO-wqKmLk"
   },
   "outputs": [],
   "source": [
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7hxru5WiJUSj"
   },
   "outputs": [],
   "source": [
    "# dataset loader\n",
    "data_loader = DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size, # 배치 크기는 100\n",
    "                                          shuffle=True, # 섞어야지. training 이 순서에 익숙해지면 안되니까!\n",
    "                                          drop_last=True) # drop_last : 마지막 배치를 버릴 것인지를 의미 (찌꺼기는 버리자.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E36Ibki7J7Y9"
   },
   "source": [
    "**drop last**\n",
    "\n",
    "drop_last를 하는 이유를 이해하기 위해서 1,000개의 데이터가 있다고 했을 때, 배치 크기가 128이라고 해봅시다. 1,000을 128로 나누면 총 7개가 나오고 나머지로 104개가 남습니다. 이때 104개를 마지막 배치로 한다고 하였을 때 128개를 충족하지 못하였으므로 104개를 그냥 버릴 수도 있습니다. 이때 마지막 배치를 버리려면 drop_last=True를 해주면 됩니다. 이는 다른 미니 배치보다 개수가 적은 마지막 배치를 경사 하강법에 사용하여 마지막 배치가 상대적으로 과대 평가되는 현상을 막아줍니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MBYQJ1y7Lk_M"
   },
   "source": [
    "## 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "sWnBGw8qTZUj",
    "outputId": "5e26b3aa-1f9a-4e68-b7eb-3a504428f9c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 기기로 학습합니다: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available() # GPU를 사용가능하면 True, 아니라면 False를 리턴\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") # GPU 사용 가능하면 사용하고 아니면 CPU 사용\n",
    "print(\"다음 기기로 학습합니다:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-HM5asMCLZrB"
   },
   "outputs": [],
   "source": [
    "# MNIST data image of shape 28 * 28 = 784 , output 은 10개 (0~10)\n",
    "linear = nn.Linear(784, 10, bias=True).to(device) \n",
    "# bias 는 편향값으로 기본적으로는 True 이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQT80CIZLjs-"
   },
   "source": [
    "**to.()**\n",
    "\n",
    "to() 함수는 연산을 어디서 수행할지를 정합니다. to() 함수는 모델의 매개변수를 지정한 장치의 메모리로 보냅니다. CPU를 사용할 경우에는 필요가 없지만, GPU를 사용하려면 to('cuda')를 해 줄 필요가 있습니다. 아무것도 지정하지 않은 경우에는 CPU 연산이라고 보면 됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hV2IA4jJTJYT"
   },
   "outputs": [],
   "source": [
    "# 비용 함수와 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss().to(device) # 내부적으로 소프트맥스 함수를 포함하고 있음.\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)\n",
    "# 앞서 소프트맥스 회귀를 배울 때는 torch.nn.functional.cross_entropy()를 사용하였으나 여기서는 torch.nn.CrossEntropyLoss()을 사용하고 있습니다. 둘 다 파이토치에서 제공하는 크로스 엔트로피 함수로 둘 다 소프트맥스 함수를 포함하고 있습니다.\n",
    "# 두개 차이는 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "colab_type": "code",
    "id": "hxbApAY7Twpu",
    "outputId": "a62dbea0-9469-4337-ae95-4ed00db7160f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.535468519\n",
      "Epoch: 0002 cost = 0.359274209\n",
      "Epoch: 0003 cost = 0.331187546\n",
      "Epoch: 0004 cost = 0.316578031\n",
      "Epoch: 0005 cost = 0.307158172\n",
      "Epoch: 0006 cost = 0.300180763\n",
      "Epoch: 0007 cost = 0.295130223\n",
      "Epoch: 0008 cost = 0.290851504\n",
      "Epoch: 0009 cost = 0.287417084\n",
      "Epoch: 0010 cost = 0.284379542\n",
      "Epoch: 0011 cost = 0.281825215\n",
      "Epoch: 0012 cost = 0.279800713\n",
      "Epoch: 0013 cost = 0.277808994\n",
      "Epoch: 0014 cost = 0.276154310\n",
      "Epoch: 0015 cost = 0.274440825\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs): # 앞서 training_epochs의 값은 15로 지정함.\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader) # len(data_loader) = 데이터의 크기\n",
    "\n",
    "    for X, Y in data_loader: # 이떄에는 굳이 batch 별로 볼 필요가 없어서 enumerate 를 쓰지 않았다.\n",
    "        # 배치 크기가 100이므로 아래의 연산에서 X는 (100, 784)의 텐서가 된다.\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        # 레이블은 원-핫 인코딩이 된 상태가 아니라 0 ~ 9의 정수.\n",
    "        Y = Y.to(device) \n",
    "\n",
    "        optimizer.zero_grad() # 계산 부분은 어짜피 gpu 로 보내진 값들을 이용해 계산하는것이므로 to 가 없어도 된다.\n",
    "        hypothesis = linear(X) \n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch # 한번 연산마다 cost 가 저장이 되고, 그 cost 에 대해서 sample 의 수로 나누어 평균 cost 를 계산한다.\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a-iNRgrPUnZp"
   },
   "source": [
    "## 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "Zi5ajsWxUEZn",
    "outputId": "30457d22-a581-4850-9ad9-c9040c579060",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8863000273704529\n",
      "Label:  8\n",
      "Prediction:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # torch.no_grad()를 하면 gradient 계산을 수행하지 않는다.\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = linear(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "\n",
    "    # MNIST 테스트 데이터에서 무작위로 하나를 뽑아서 예측을 해본다\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = linear(X_single_data) # 이떄의 linear 은 이미 학습이 끝난 w,b 를 적용한 model\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n",
    "\n",
    "    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNCnJCq5wuDct1IVcJgk6DJ",
   "collapsed_sections": [],
   "name": "Linear regression",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336.458px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
